Ollama: Open Source tool for running LLMs locally.

Ollama  provides a convenient way to download and run advanced AI models (like LLaMA, Mistral, etc.) 
directly on your machine instead of relying on cloud APIs. This approach is appealing for developers, 
researchers, and businesses that care about data control and privacy, since no data needs to leave the 
local environment . By running models offline, you maintain full ownership of your data and eliminate 
potential security risks of sending sensitive information to third-party servers. 

Another benefit of local execution is reduced latency – responses can be faster and more reliable 
without network delays . Overall, Ollama’s purpose is to make it easy to experiment with and use advanced 
language models on personal or on-premise systems, giving you more control over both the models and your data.

Benefits:
    - Data Privacy / Security
    - Offline Capability
    - Low Latency / Performance
    - Cost Savings
    - Control and Customization


   Installation:
   curl -fsSL https://ollama.com/install.sh | sh 

##   Check install:
##   ollama --version

Ollama_Getting_Started/> ollama --version
ollama version is 0.11.6

## CLI usage 
## 
Ollama_Getting_Started/> ollama
Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model
  show        Show information for a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.

## alias: let's shorten this up
alias olm=ollama

## use ss to find the port / process
## -t : show tcp connections
## -u : show udp connections
## -l : show listening sockets
## -p : show PID and program name of process ownig the socket
## -n : show numerical addresses and port numbers and program name of process ownig the socket

> sudo ss -tulpn
Netid   State    Recv-Q   Send-Q      Local Address:Port       Peer Address:Port   Process                                                       
udp     UNCONN   0        0                 0.0.0.0:47165           0.0.0.0:*
....                                                                    
tcp     LISTEN   0        4096            127.0.0.1:11434           0.0.0.0:*       users:(("ollama",pid=383423,fd=3)) 



## ollama server
##   - launches Ollama server process that powers API and model loading























.